{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking through N-Grams as Factors\n",
    "### (Started July 2, 2019)\n",
    "\n",
    "## Introduction\n",
    "After seeing the potentially strong results from filtering the articles by \"phase\" then by a second keyword, it became clear that there could be some other interesting groupings of words.\n",
    "\n",
    "The intuition is that there are likely to be certain groups of words that could result in statistically significant risk-adjusted returns.\n",
    "\n",
    "The high-level approach will be:\n",
    "1. Reduce the words in the corpus of text as much as possible. The key here is to remove as many irrelevant words.\n",
    "2. For each set of n-grams:\n",
    "    * Filter the article Data Frame using the words in the n-gram\n",
    "    * Get the Return metrics for the filtered articles\n",
    "3. Calculate and sort by the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "1. [\"Imports, Settings and Data Loading\"](#1)\n",
    "2. [\"Text Cleaning and Feature Reduction\"](#2)\n",
    "3. [\"Build N-Gram Functionality](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Imports, Settings and Data Loading\n",
    "\n",
    "Note: All of this section came from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Standard Libraries\n",
    "from itertools import combinations\n",
    "\n",
    "# Numerical Libraries\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Visual Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local Package Libraries\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.data.make_dataset import *\n",
    "from src.features.general_helper_functions import *\n",
    "from src.features.text_cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# Stop the warnings for chain in pandas...\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, watchlist_raw, stock_prices_raw = get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Added the cleaning and formatting functions to make_dataset.py - July 2, 2019)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ACAD    ACHC  ACOR  ADUS  AERI  AGIO  AIMT  AKCA  AKRX  ALDR  \\\n",
      "1998-01-02   NaN  3.5958   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1998-01-05   NaN  3.4911   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1998-01-06   NaN  3.5958   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1998-01-07   NaN  3.5958   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "1998-01-08   NaN  3.5958   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "            ...   VRAY  VREX  WMGI  WVE  XENT  XLRN  XNCR  XON  YI  ZGNX  \n",
      "1998-01-02  ...    NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN NaN   NaN  \n",
      "1998-01-05  ...    NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN NaN   NaN  \n",
      "1998-01-06  ...    NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN NaN   NaN  \n",
      "1998-01-07  ...    NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN NaN   NaN  \n",
      "1998-01-08  ...    NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN NaN   NaN  \n",
      "\n",
      "[5 rows x 197 columns]\n",
      "        time                                              title ticker  \\\n",
      "0 2019-06-04  acadia pharmaceuticals to present at the goldm...   ACAD   \n",
      "1 2019-05-18  acadia pharmaceuticals to present phase 2 clar...   ACAD   \n",
      "2 2019-05-15  fastest growing companies/startups in san fran...   ACAD   \n",
      "3 2019-05-07  acadia pharmaceuticals to present at the bank ...   ACAD   \n",
      "4 2019-05-02  alzheimer's disease: pipeline review, develope...   ACAD   \n",
      "\n",
      "                                             article  \n",
      "0   san diego--(business wire)--acadia pharmaceut...  \n",
      "1   san diego--(business wire)--acadia pharmaceut...  \n",
      "2   boulder, colo.--(business wire)--growjo annou...  \n",
      "3   san diego--(business wire)--acadia pharmaceut...  \n",
      "4   dublin--(business wire)--the \"alzheimer's dis...  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R_0</th>\n",
       "      <th>R_1</th>\n",
       "      <th>R_2</th>\n",
       "      <th>R_3</th>\n",
       "      <th>R_4</th>\n",
       "      <th>R_5</th>\n",
       "      <th>R_6</th>\n",
       "      <th>R_7</th>\n",
       "      <th>R_8</th>\n",
       "      <th>R_9</th>\n",
       "      <th>...</th>\n",
       "      <th>R_19</th>\n",
       "      <th>R_20</th>\n",
       "      <th>R_21</th>\n",
       "      <th>R_22</th>\n",
       "      <th>R_23</th>\n",
       "      <th>R_24</th>\n",
       "      <th>R_25</th>\n",
       "      <th>R_26</th>\n",
       "      <th>R_27</th>\n",
       "      <th>R_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016568</td>\n",
       "      <td>-0.017357</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.011045</td>\n",
       "      <td>-0.042998</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010794</td>\n",
       "      <td>-0.035081</td>\n",
       "      <td>-0.065921</td>\n",
       "      <td>-0.015420</td>\n",
       "      <td>-0.030840</td>\n",
       "      <td>-0.023130</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>-0.052043</td>\n",
       "      <td>-0.040093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011951</td>\n",
       "      <td>-0.064765</td>\n",
       "      <td>-0.020046</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>-0.008096</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>-0.045719</td>\n",
       "      <td>-0.008136</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>-0.005812</td>\n",
       "      <td>-0.030221</td>\n",
       "      <td>-0.061217</td>\n",
       "      <td>-0.010461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001550</td>\n",
       "      <td>-0.034870</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>-0.005812</td>\n",
       "      <td>-0.007361</td>\n",
       "      <td>-0.009299</td>\n",
       "      <td>-0.006974</td>\n",
       "      <td>-0.060054</td>\n",
       "      <td>-0.015110</td>\n",
       "      <td>0.004649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.050588</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.025490</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>-0.034118</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.017255</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059216</td>\n",
       "      <td>-0.043529</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>-0.023137</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.005098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        R_0       R_1       R_2       R_3       R_4       R_5       R_6  \\\n",
       "0  0.016568 -0.017357  0.019329  0.012229  0.010651  0.008679  0.011045   \n",
       "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2 -0.010794 -0.035081 -0.065921 -0.015420 -0.030840 -0.023130  0.000771   \n",
       "3  0.013173  0.006587  0.001162 -0.045719 -0.008136  0.005037 -0.005812   \n",
       "4  0.040000  0.050588  0.012157  0.025490  0.018824  0.013333 -0.034118   \n",
       "\n",
       "        R_7       R_8       R_9    ...         R_19      R_20      R_21  \\\n",
       "0 -0.042998  0.002761  0.022880    ...          NaN       NaN       NaN   \n",
       "1       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "2  0.007710 -0.052043 -0.040093    ...    -0.011951 -0.064765 -0.020046   \n",
       "3 -0.030221 -0.061217 -0.010461    ...    -0.001550 -0.034870  0.001162   \n",
       "4  0.003922  0.017255  0.006275    ...    -0.059216 -0.043529 -0.005882   \n",
       "\n",
       "       R_22      R_23      R_24      R_25      R_26      R_27      R_28  \n",
       "0       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "1       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "2 -0.000386  0.026214 -0.008096  0.002313       NaN       NaN       NaN  \n",
       "3 -0.005812 -0.007361 -0.009299 -0.006974 -0.060054 -0.015110  0.004649  \n",
       "4  0.010588 -0.023137  0.013333  0.006275  0.004706  0.002745  0.005098  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = clean_and_open_business_wire_data_01(None)\n",
    "article_df.time = pd.to_datetime(article_df.time)\n",
    "\n",
    "# Watchlist\n",
    "watchlist_df = clean_and_format_watchlist(watchlist_raw, article_df.ticker.unique())\n",
    "\n",
    "\n",
    "# Stock Prices\n",
    "prices_df = clean_and_format_prices(stock_prices_raw, article_df.ticker.unique())\n",
    "\n",
    "# Return Window\n",
    "return_window = compute_return_window(article_df, prices_df, n_window=30)\n",
    "\n",
    "return_window.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Feature Reduction\n",
    "\n",
    "Note: The first block is also from the previous Notebook. Should probably add these to src.\n",
    "\n",
    "*(Added to src: nlp_functions.py - July 2, 2019)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_frequency(word_list, df):\n",
    "    d = {word: sum([1 if word in article else 0 for article in df.title.values])/df.shape[0] for word in word_list}\n",
    "    \n",
    "    return pd.Series(d, index = d.keys()).sort_values(ascending=False)\n",
    "\n",
    "def get_list_of_words(articles, cut_off):    \n",
    "    combined_titles = \" \".join(articles.title.values.tolist())\n",
    "\n",
    "    set_of_words = list(set(combined_titles.split(\" \")))\n",
    "    \n",
    "    set_of_words = [word for word in set_of_words if len(word) > 3]\n",
    "\n",
    "    word_frequency = calculate_word_frequency(set_of_words, articles)\n",
    "    set_of_words = word_frequency.loc[word_frequency > cut_off].index\n",
    "    return set_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "article_df = clean_text(article_df, \"title\")\n",
    "\n",
    "list_of_words = get_list_of_words(article_df, cut_off=0.01)\n",
    "\n",
    "print(len(list_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can go through the article titles and filter out all words that are not in the list_of_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_sublist_words(text, list_of_words):\n",
    "    return \" \".join([word for word in text.split(\" \") if word in list_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.title = article_df.title.apply(keep_sublist_words, args=(list_of_words,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can drop the columns \"ticker\" and \"article\" as they won't be needed. \n",
    "\n",
    "Further, it will be useful to have a column for each word with a value of True or False if the word exists in the title or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = article_df.drop([\"ticker\", \"article\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8433, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in list_of_words:\n",
    "    article_df[word] = article_df.title.str.contains(word)\n",
    "    \n",
    "article_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>mark</th>\n",
       "      <th>market</th>\n",
       "      <th>search</th>\n",
       "      <th>research</th>\n",
       "      <th>chan</th>\n",
       "      <th>hand</th>\n",
       "      <th>researchandmarkets</th>\n",
       "      <th>global</th>\n",
       "      <th>...</th>\n",
       "      <th>american</th>\n",
       "      <th>stage</th>\n",
       "      <th>unite</th>\n",
       "      <th>micro</th>\n",
       "      <th>strategy</th>\n",
       "      <th>cure</th>\n",
       "      <th>administration</th>\n",
       "      <th>light</th>\n",
       "      <th>receive</th>\n",
       "      <th>next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>pharmaceutical present annual global healthcar...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-18</td>\n",
       "      <td>pharmaceutical present phase result treatment ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>grow company award</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>pharmaceutical present america health care con...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>disease pipeline review insight researchandmar...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time                                              title   mark  \\\n",
       "0 2019-06-04  pharmaceutical present annual global healthcar...  False   \n",
       "1 2019-05-18  pharmaceutical present phase result treatment ...  False   \n",
       "2 2019-05-15                                 grow company award  False   \n",
       "3 2019-05-07  pharmaceutical present america health care con...  False   \n",
       "4 2019-05-02  disease pipeline review insight researchandmar...   True   \n",
       "\n",
       "   market  search  research   chan   hand  researchandmarkets  global  ...    \\\n",
       "0   False   False     False  False  False               False    True  ...     \n",
       "1   False   False     False  False  False               False   False  ...     \n",
       "2   False   False     False  False  False               False   False  ...     \n",
       "3   False   False     False  False  False               False   False  ...     \n",
       "4    True    True      True   True   True                True   False  ...     \n",
       "\n",
       "   american  stage  unite  micro  strategy   cure  administration  light  \\\n",
       "0     False  False  False  False     False  False           False  False   \n",
       "1      True  False  False  False     False  False           False  False   \n",
       "2     False  False  False  False     False  False           False  False   \n",
       "3     False  False  False  False     False  False           False  False   \n",
       "4     False  False  False  False     False  False           False  False   \n",
       "\n",
       "   receive   next  \n",
       "0    False  False  \n",
       "1    False  False  \n",
       "2    False  False  \n",
       "3    False  False  \n",
       "4    False  False  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## Build N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(words, n):\n",
    "    return combinations(words, n)\n",
    "\n",
    "def get_ngram_articles(articles, ngram_tuple):\n",
    "    temp_articles = articles.copy()\n",
    "    for word in ngram_tuple:\n",
    "        temp_articles = temp_articles.loc[temp_articles[word]]\n",
    "    return temp_articles.index.tolist()\n",
    "\n",
    "def get_dict_ngram_articles(articles, list_of_words):\n",
    "    ngram = get_n_gram(list_of_words, 2)\n",
    "    return {word_tuple: get_ngram_articles(article_df, word_tuple) for word_tuple in ngram}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32131\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_linking_ngram_to_indexes = get_dict_ngram_articles(article_df, list_of_words)\n",
    "\n",
    "\n",
    "print(len(dict_linking_ngram_to_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## Iterate and Calculate Metrics per N-Gram\n",
    "\n",
    "For each n-gram will need to collect the stock returns for our window, then calculate the return metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original function is from Notebook 02 and converted to work with n-grams\n",
    "\n",
    "def get_return_details_per_word(article_df, return_df, word_list, holding_period, cut_off=0.05):\n",
    "    set_of_ngrams = get_n_gram(word_list, 2)\n",
    "    \n",
    "    res_dict = {}\n",
    "    for ngram in set_of_ngrams:\n",
    "        event_id_list = get_ngram_articles(article_df, ngram)\n",
    "        #print(event_id_list)\n",
    "        returns = return_df[\"R_{}\".format(holding_period - 1)].iloc[event_id_list].dropna()\n",
    "        \n",
    "        # note will use the string of the ngram for now as I need to learn more about\n",
    "        # multi-indexing with pandas\n",
    "        res_dict[str(ngram)] = [\n",
    "            np.mean(returns), \n",
    "            np.std(returns), \n",
    "            skew(returns.values), \n",
    "            kurtosis(returns.values),\n",
    "            returns.shape[0]/article_df.shape[0]\n",
    "        ]\n",
    "    \n",
    "    cols = [\"return\", \"dev\", \"skew\", \"kurt\", \"freq_occurance\"]\n",
    "    \n",
    "    return pd.DataFrame(res_dict, index=cols).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = get_return_details_per_word(article_df, \n",
    "                                     return_window, \n",
    "                                     list_of_words, \n",
    "                                     holding_period=20, \n",
    "                                     cut_off=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(row, holding_period, annual_risk_free_rate):\n",
    "    scale_param = 252 / holding_period # This will be used to annualize the expected return \n",
    "                                       # and the deviation\n",
    "    num = (scale_param * row[\"return\"] - annual_risk_free_rate) \n",
    "    den = (np.sqrt(scale_param) * row[\"dev\"])\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Sharpe Ratios:\n",
      "                          return  dev  skew  kurt  freq_occurance  \\\n",
      "('receive', 'next')     0.525436  0.0   0.0  -3.0        0.000119   \n",
      "('part', 'genetic')     0.513262  0.0   0.0  -3.0        0.000119   \n",
      "('part', 'group')       0.052409  0.0   0.0  -3.0        0.000119   \n",
      "('part', 'pain')        0.564841  0.0   0.0  -3.0        0.000119   \n",
      "('part', 'executive')   0.033368  0.0   0.0  -3.0        0.000119   \n",
      "('view', 'invest')      0.052409  0.0   0.0  -3.0        0.000119   \n",
      "('view', 'care')        0.100343  0.0   0.0  -3.0        0.000119   \n",
      "('part', 'exec')        0.033368  0.0   0.0  -3.0        0.000119   \n",
      "('part', 'state')       0.060168  0.0   0.0  -3.0        0.000119   \n",
      "('view', 'conference')  0.043609  0.0   0.0  -3.0        0.000119   \n",
      "\n",
      "                        sharpe_ratio  \n",
      "('receive', 'next')              inf  \n",
      "('part', 'genetic')              inf  \n",
      "('part', 'group')                inf  \n",
      "('part', 'pain')                 inf  \n",
      "('part', 'executive')            inf  \n",
      "('view', 'invest')               inf  \n",
      "('view', 'care')                 inf  \n",
      "('part', 'exec')                 inf  \n",
      "('part', 'state')                inf  \n",
      "('view', 'conference')           inf  \n",
      "Bottom 10 Sharpe Ratios:\n",
      "                             return  dev  skew  kurt  freq_occurance  \\\n",
      "('strategy', 'receive')         NaN  NaN   NaN   NaN             0.0   \n",
      "('strategy', 'next')            NaN  NaN   NaN   NaN             0.0   \n",
      "('cure', 'administration')      NaN  NaN   NaN   NaN             0.0   \n",
      "('cure', 'light')               NaN  NaN   NaN   NaN             0.0   \n",
      "('cure', 'receive')             NaN  NaN   NaN   NaN             0.0   \n",
      "('cure', 'next')                NaN  NaN   NaN   NaN             0.0   \n",
      "('administration', 'light')     NaN  NaN   NaN   NaN             0.0   \n",
      "('administration', 'next')      NaN  NaN   NaN   NaN             0.0   \n",
      "('light', 'receive')            NaN  NaN   NaN   NaN             0.0   \n",
      "('light', 'next')               NaN  NaN   NaN   NaN             0.0   \n",
      "\n",
      "                             sharpe_ratio  \n",
      "('strategy', 'receive')               NaN  \n",
      "('strategy', 'next')                  NaN  \n",
      "('cure', 'administration')            NaN  \n",
      "('cure', 'light')                     NaN  \n",
      "('cure', 'receive')                   NaN  \n",
      "('cure', 'next')                      NaN  \n",
      "('administration', 'light')           NaN  \n",
      "('administration', 'next')            NaN  \n",
      "('light', 'receive')                  NaN  \n",
      "('light', 'next')                     NaN  \n"
     ]
    }
   ],
   "source": [
    "res_df[\"sharpe_ratio\"] = res_df.apply(sharpe_ratio, args=(20, 0.025,), axis=1)\n",
    "res_df.sort_values(\"sharpe_ratio\", ascending=False, inplace=True)\n",
    "\n",
    "print(\"Top 10 Sharpe Ratios:\")\n",
    "print(res_df.head(10))\n",
    "\n",
    "print(\"Bottom 10 Sharpe Ratios:\")\n",
    "print(res_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I also need to have a cutoff for the n-grams! The issue here is that the frequency occurred is basically zero for a lot of these results. will use a similar functionality as with the individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding frequency cut-off for n-grams\n",
    "\n",
    "def get_return_details_per_word(article_df, return_df, word_list, holding_period, cut_off=0.05):\n",
    "    set_of_ngrams = get_n_gram(word_list, 2)\n",
    "    \n",
    "    res_dict = {}\n",
    "    for ngram in set_of_ngrams:\n",
    "        event_id_list = get_ngram_articles(article_df, ngram)\n",
    "        \n",
    "        #print(event_id_list)\n",
    "        size_prior_to_nan_removal = len(event_id_list)\n",
    "        returns = return_df[\"R_{}\".format(holding_period - 1)].iloc[event_id_list].dropna()\n",
    "        num_removed = size_prior_to_nan_removal - returns.shape[0]\n",
    "        \n",
    "        freq = returns.shape[0]/article_df.shape[0]\n",
    "        if freq >= cut_off:\n",
    "        \n",
    "            res_dict[ngram] = [\n",
    "                np.mean(returns), \n",
    "                np.std(returns), \n",
    "                skew(returns.values), \n",
    "                kurtosis(returns.values),\n",
    "                freq,\n",
    "                num_removed\n",
    "            ]\n",
    "    \n",
    "    cols = [\"return\", \"dev\", \"skew\", \"kurt\", \"freq_occurance\", \"samples_removed\"]\n",
    "    \n",
    "    return pd.DataFrame(res_dict, index=cols).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = get_return_details_per_word(article_df, \n",
    "                                     return_window, \n",
    "                                     list_of_words, \n",
    "                                     holding_period=20, \n",
    "                                     cut_off=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[\"sharpe_ratio\"] = res_df.apply(sharpe_ratio, args=(20, 0.025,), axis=1)\n",
    "res_df.sort_values(\"sharpe_ratio\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Sharpe Ratios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>return</th>\n",
       "      <th>dev</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>freq_occurance</th>\n",
       "      <th>samples_removed</th>\n",
       "      <th>sharpe_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>view</th>\n",
       "      <th>review</th>\n",
       "      <td>0.139943</td>\n",
       "      <td>0.236180</td>\n",
       "      <td>0.486534</td>\n",
       "      <td>-1.209771</td>\n",
       "      <td>0.106012</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.073445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <th>view</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <th>review</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research</th>\n",
       "      <th>review</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">search</th>\n",
       "      <th>review</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>view</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <th>view</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">market</th>\n",
       "      <th>review</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>view</th>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>-1.198074</td>\n",
       "      <td>0.104352</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.048560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <th>quarter</th>\n",
       "      <td>0.134578</td>\n",
       "      <td>0.231313</td>\n",
       "      <td>0.567179</td>\n",
       "      <td>-1.115436</td>\n",
       "      <td>0.062611</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.034747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    return       dev      skew      kurt  freq_occurance  \\\n",
       "view     review   0.139943  0.236180  0.486534 -1.209771        0.106012   \n",
       "research view     0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "mark     review   0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "research review   0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "search   review   0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "         view     0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "mark     view     0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "market   review   0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "         view     0.137863  0.235445  0.494276 -1.198074        0.104352   \n",
       "result   quarter  0.134578  0.231313  0.567179 -1.115436        0.062611   \n",
       "\n",
       "                  samples_removed  sharpe_ratio  \n",
       "view     review              52.0      2.073445  \n",
       "research view                49.0      2.048560  \n",
       "mark     review              49.0      2.048560  \n",
       "research review              49.0      2.048560  \n",
       "search   review              49.0      2.048560  \n",
       "         view                49.0      2.048560  \n",
       "mark     view                49.0      2.048560  \n",
       "market   review              49.0      2.048560  \n",
       "         view                49.0      2.048560  \n",
       "result   quarter             17.0      2.034747  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 Sharpe Ratios:\")\n",
    "res_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 Sharpe Ratios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>return</th>\n",
       "      <th>dev</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>freq_occurance</th>\n",
       "      <th>samples_removed</th>\n",
       "      <th>sharpe_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <th>researchandmarkets</th>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.200197</td>\n",
       "      <td>1.005750</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.263607</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1.308986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">chan</th>\n",
       "      <th>hand</th>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.200197</td>\n",
       "      <td>1.005750</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.263607</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1.308986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>researchandmarkets</th>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.200197</td>\n",
       "      <td>1.005750</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.263607</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1.308986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <th>hand</th>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.200197</td>\n",
       "      <td>1.005750</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.263607</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1.308986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hand</th>\n",
       "      <th>pipeline</th>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.182138</td>\n",
       "      <td>1.151931</td>\n",
       "      <td>0.544994</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.106118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>researchandmarkets</th>\n",
       "      <th>pipeline</th>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.182138</td>\n",
       "      <td>1.151931</td>\n",
       "      <td>0.544994</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.106118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chan</th>\n",
       "      <th>pipeline</th>\n",
       "      <td>0.058741</td>\n",
       "      <td>0.182138</td>\n",
       "      <td>1.151931</td>\n",
       "      <td>0.544994</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.106118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hand</th>\n",
       "      <th>line</th>\n",
       "      <td>0.058633</td>\n",
       "      <td>0.182008</td>\n",
       "      <td>1.154213</td>\n",
       "      <td>0.552082</td>\n",
       "      <td>0.072098</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.104804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chan</th>\n",
       "      <th>line</th>\n",
       "      <td>0.058633</td>\n",
       "      <td>0.182008</td>\n",
       "      <td>1.154213</td>\n",
       "      <td>0.552082</td>\n",
       "      <td>0.072098</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.104804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>researchandmarkets</th>\n",
       "      <th>line</th>\n",
       "      <td>0.058633</td>\n",
       "      <td>0.182008</td>\n",
       "      <td>1.154213</td>\n",
       "      <td>0.552082</td>\n",
       "      <td>0.072098</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.104804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         return       dev      skew      kurt  \\\n",
       "mark               researchandmarkets  0.075810  0.200197  1.005750  0.001682   \n",
       "chan               hand                0.075810  0.200197  1.005750  0.001682   \n",
       "                   researchandmarkets  0.075810  0.200197  1.005750  0.001682   \n",
       "mark               hand                0.075810  0.200197  1.005750  0.001682   \n",
       "hand               pipeline            0.058741  0.182138  1.151931  0.544994   \n",
       "researchandmarkets pipeline            0.058741  0.182138  1.151931  0.544994   \n",
       "chan               pipeline            0.058741  0.182138  1.151931  0.544994   \n",
       "hand               line                0.058633  0.182008  1.154213  0.552082   \n",
       "chan               line                0.058633  0.182008  1.154213  0.552082   \n",
       "researchandmarkets line                0.058633  0.182008  1.154213  0.552082   \n",
       "\n",
       "                                       freq_occurance  samples_removed  \\\n",
       "mark               researchandmarkets        0.263607            211.0   \n",
       "chan               hand                      0.263607            211.0   \n",
       "                   researchandmarkets        0.263607            211.0   \n",
       "mark               hand                      0.263607            211.0   \n",
       "hand               pipeline                  0.071979             53.0   \n",
       "researchandmarkets pipeline                  0.071979             53.0   \n",
       "chan               pipeline                  0.071979             53.0   \n",
       "hand               line                      0.072098             53.0   \n",
       "chan               line                      0.072098             53.0   \n",
       "researchandmarkets line                      0.072098             53.0   \n",
       "\n",
       "                                       sharpe_ratio  \n",
       "mark               researchandmarkets      1.308986  \n",
       "chan               hand                    1.308986  \n",
       "                   researchandmarkets      1.308986  \n",
       "mark               hand                    1.308986  \n",
       "hand               pipeline                1.106118  \n",
       "researchandmarkets pipeline                1.106118  \n",
       "chan               pipeline                1.106118  \n",
       "hand               line                    1.104804  \n",
       "chan               line                    1.104804  \n",
       "researchandmarkets line                    1.104804  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Bottom 10 Sharpe Ratios:\")\n",
    "res_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Summary\n",
    "\n",
    "Similar to the previous notebook, these results seem to be oddly amazing.\n",
    "\n",
    "A Sharpe Ratio of 2 and better is classified as really good, and the occurance of the n-grams of words is high enough that is seems to be significant.\n",
    "\n",
    "A few notes:\n",
    "* The high Sharpe Ratio events also have a low positive skew and a highly negative kurtosis. This could imply that there are a a few very large negative results. If trading, this could lend well to option trading where there is a convex pay-off structure.\n",
    "* The main research issues in this will be similar to those in the previous notebook as I still have not collected any additional data.\n",
    "* I notice also that in the top 10, there are quite a few ngrams containing the words \"view\", \"review, \"market\". I wonder if there is a reason for this. I will take a manual peek at those titles to see what is happening there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('view', 'review'), ('research', 'view'), ('mark', 'review'), ('research', 'review'), ('search', 'review'), ('search', 'view'), ('mark', 'view'), ('market', 'review'), ('market', 'view'), ('result', 'quarter')]\n"
     ]
    }
   ],
   "source": [
    "ngrams_top_10 = res_df.head(10).index.tolist()\n",
    "\n",
    "print(ngrams_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline review stage development drug action administration molecule type researchandmarkets \n",
      "\n",
      "global syndrome report pipeline review pharma company drug profile research market \n",
      "\n",
      "global therapeutic pipeline report review company drug profile research market \n",
      "\n",
      "pipeline review player analysis researchandmarkets \n",
      "\n",
      "therapeutic pipeline review research market \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a, b = ngrams_top_10[0]\n",
    "\n",
    "article_sample = article_df.loc[article_df.title.str.contains(a and b)].sample(5)\n",
    "\n",
    "for _, article in article_sample.iterrows():\n",
    "    print(article.title, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a really fundamental issue here that needs to be fixed. \n",
    "\n",
    "When searching for key words the way that is done here, view is a substring of review. Anytime that the word review happens, view will also happen. This needs to be changed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
