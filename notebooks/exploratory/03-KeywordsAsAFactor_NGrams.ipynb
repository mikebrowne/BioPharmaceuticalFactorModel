{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking through N-Grams as Factors\n",
    "### (Started July 2, 2019)\n",
    "\n",
    "## Introduction\n",
    "After seeing the potentially strong results from filtering the articles by \"phase\" then by a second keyword, it became clear that there could be some other interesting groupings of words.\n",
    "\n",
    "The intuition is that there are likely to be certain groups of words that could result in statistically significant risk-adjusted returns.\n",
    "\n",
    "The high-level approach will be:\n",
    "1. Reduce the words in the corpus of text as much as possible. The key here is to remove as many irrelevant words.\n",
    "2. For each set of n-grams:\n",
    "    * Filter the article Data Frame using the words in the n-gram\n",
    "    * Get the Return metrics for the filtered articles\n",
    "3. Calculate and sort by the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "1. [\"Imports, Settings and Data Loading\"](#1)\n",
    "2. [\"Text Cleaning and Feature Reduction\"](#2)\n",
    "3. [\"Build N-Gram Functionality](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Imports, Settings and Data Loading\n",
    "\n",
    "Note: All of this section came from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Standard Libraries\n",
    "from itertools import combinations\n",
    "\n",
    "# Numerical Libraries\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Visual Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Local Package Libraries\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.data.make_dataset import clean_and_open_business_wire_data_01, get_raw_data\n",
    "from src.features.general_helper_functions import GetPrices\n",
    "from src.features.nlp_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# Stop the warnings for chain in pandas...\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, watchlist_raw, stock_prices_raw = get_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  (721, 4)\n",
      "Final size:  (197, 4)\n",
      "Original size:  (5402, 208)\n",
      "Final size:  (5402, 197)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACAD</th>\n",
       "      <th>ACHC</th>\n",
       "      <th>ACOR</th>\n",
       "      <th>ADUS</th>\n",
       "      <th>AERI</th>\n",
       "      <th>AGIO</th>\n",
       "      <th>AIMT</th>\n",
       "      <th>AKCA</th>\n",
       "      <th>AKRX</th>\n",
       "      <th>ALDR</th>\n",
       "      <th>...</th>\n",
       "      <th>VRAY</th>\n",
       "      <th>VREX</th>\n",
       "      <th>WMGI</th>\n",
       "      <th>WVE</th>\n",
       "      <th>XENT</th>\n",
       "      <th>XLRN</th>\n",
       "      <th>XNCR</th>\n",
       "      <th>XON</th>\n",
       "      <th>YI</th>\n",
       "      <th>ZGNX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-17</th>\n",
       "      <td>25.42</td>\n",
       "      <td>34.00</td>\n",
       "      <td>7.68</td>\n",
       "      <td>72.56</td>\n",
       "      <td>33.29</td>\n",
       "      <td>50.66</td>\n",
       "      <td>20.54</td>\n",
       "      <td>24.39</td>\n",
       "      <td>4.34</td>\n",
       "      <td>12.45</td>\n",
       "      <td>...</td>\n",
       "      <td>8.95</td>\n",
       "      <td>28.21</td>\n",
       "      <td>31.31</td>\n",
       "      <td>27.43</td>\n",
       "      <td>24.45</td>\n",
       "      <td>38.36</td>\n",
       "      <td>32.55</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.75</td>\n",
       "      <td>40.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-18</th>\n",
       "      <td>25.93</td>\n",
       "      <td>34.20</td>\n",
       "      <td>7.66</td>\n",
       "      <td>72.92</td>\n",
       "      <td>33.84</td>\n",
       "      <td>51.91</td>\n",
       "      <td>20.20</td>\n",
       "      <td>23.98</td>\n",
       "      <td>4.48</td>\n",
       "      <td>11.73</td>\n",
       "      <td>...</td>\n",
       "      <td>9.13</td>\n",
       "      <td>28.95</td>\n",
       "      <td>31.69</td>\n",
       "      <td>27.92</td>\n",
       "      <td>23.97</td>\n",
       "      <td>40.62</td>\n",
       "      <td>34.54</td>\n",
       "      <td>8.60</td>\n",
       "      <td>6.62</td>\n",
       "      <td>39.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-19</th>\n",
       "      <td>26.62</td>\n",
       "      <td>34.04</td>\n",
       "      <td>7.54</td>\n",
       "      <td>75.89</td>\n",
       "      <td>32.89</td>\n",
       "      <td>51.42</td>\n",
       "      <td>20.07</td>\n",
       "      <td>22.80</td>\n",
       "      <td>4.65</td>\n",
       "      <td>11.55</td>\n",
       "      <td>...</td>\n",
       "      <td>9.11</td>\n",
       "      <td>29.26</td>\n",
       "      <td>31.88</td>\n",
       "      <td>26.93</td>\n",
       "      <td>24.16</td>\n",
       "      <td>39.92</td>\n",
       "      <td>34.65</td>\n",
       "      <td>8.55</td>\n",
       "      <td>6.62</td>\n",
       "      <td>40.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-20</th>\n",
       "      <td>25.73</td>\n",
       "      <td>34.52</td>\n",
       "      <td>7.28</td>\n",
       "      <td>74.91</td>\n",
       "      <td>32.13</td>\n",
       "      <td>51.30</td>\n",
       "      <td>20.24</td>\n",
       "      <td>22.90</td>\n",
       "      <td>4.74</td>\n",
       "      <td>11.54</td>\n",
       "      <td>...</td>\n",
       "      <td>8.92</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.48</td>\n",
       "      <td>26.86</td>\n",
       "      <td>23.62</td>\n",
       "      <td>40.85</td>\n",
       "      <td>35.10</td>\n",
       "      <td>8.01</td>\n",
       "      <td>6.62</td>\n",
       "      <td>40.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-21</th>\n",
       "      <td>26.00</td>\n",
       "      <td>34.72</td>\n",
       "      <td>7.49</td>\n",
       "      <td>74.63</td>\n",
       "      <td>30.91</td>\n",
       "      <td>50.78</td>\n",
       "      <td>20.00</td>\n",
       "      <td>24.26</td>\n",
       "      <td>4.78</td>\n",
       "      <td>11.60</td>\n",
       "      <td>...</td>\n",
       "      <td>8.72</td>\n",
       "      <td>29.21</td>\n",
       "      <td>30.09</td>\n",
       "      <td>26.18</td>\n",
       "      <td>23.15</td>\n",
       "      <td>40.18</td>\n",
       "      <td>35.16</td>\n",
       "      <td>7.57</td>\n",
       "      <td>6.49</td>\n",
       "      <td>40.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ACAD   ACHC  ACOR   ADUS   AERI   AGIO   AIMT   AKCA  AKRX  \\\n",
       "2019-06-17  25.42  34.00  7.68  72.56  33.29  50.66  20.54  24.39  4.34   \n",
       "2019-06-18  25.93  34.20  7.66  72.92  33.84  51.91  20.20  23.98  4.48   \n",
       "2019-06-19  26.62  34.04  7.54  75.89  32.89  51.42  20.07  22.80  4.65   \n",
       "2019-06-20  25.73  34.52  7.28  74.91  32.13  51.30  20.24  22.90  4.74   \n",
       "2019-06-21  26.00  34.72  7.49  74.63  30.91  50.78  20.00  24.26  4.78   \n",
       "\n",
       "             ALDR  ...    VRAY   VREX   WMGI    WVE   XENT   XLRN   XNCR  \\\n",
       "2019-06-17  12.45  ...    8.95  28.21  31.31  27.43  24.45  38.36  32.55   \n",
       "2019-06-18  11.73  ...    9.13  28.95  31.69  27.92  23.97  40.62  34.54   \n",
       "2019-06-19  11.55  ...    9.11  29.26  31.88  26.93  24.16  39.92  34.65   \n",
       "2019-06-20  11.54  ...    8.92  29.57  30.48  26.86  23.62  40.85  35.10   \n",
       "2019-06-21  11.60  ...    8.72  29.21  30.09  26.18  23.15  40.18  35.16   \n",
       "\n",
       "             XON    YI   ZGNX  \n",
       "2019-06-17  7.60  6.75  40.30  \n",
       "2019-06-18  8.60  6.62  39.79  \n",
       "2019-06-19  8.55  6.62  40.37  \n",
       "2019-06-20  8.01  6.62  40.91  \n",
       "2019-06-21  7.57  6.49  40.62  \n",
       "\n",
       "[5 rows x 197 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = clean_and_open_business_wire_data_01(None)\n",
    "article_df.time = pd.to_datetime(article_df.time)\n",
    "\n",
    "# Watchlist\n",
    "\n",
    "# 0: Create a copy of the data\n",
    "watchlist_df = watchlist_raw.copy()\n",
    "print(\"Original size: \", watchlist_df.shape)\n",
    "\n",
    "# 1: Get a list of the unique companies that have scraped article data\n",
    "unique_companies = article_df.ticker.unique()\n",
    "\n",
    "# 2: Keep only the companies from the list\n",
    "watchlist_df = watchlist_df.loc[watchlist_df.Ticker.isin(unique_companies)]\n",
    "print(\"Final size: \", watchlist_df.shape)\n",
    "\n",
    "watchlist_df.columns = [\"ticker\", \"marketcap\", \"sector\", \"exchange\"]\n",
    "\n",
    "watchlist_df.head()\n",
    "\n",
    "\n",
    "# Stock Prices\n",
    "\n",
    "# 0: Make a copy of the stock prices here\n",
    "prices_df = stock_prices_raw.copy()\n",
    "print(\"Original size: \", prices_df.shape)\n",
    "\n",
    "# 1: Reduce the new copy of prices to only the companies under our scope\n",
    "prices_df = prices_df[unique_companies]\n",
    "print(\"Final size: \", prices_df.shape)\n",
    "\n",
    "# 2: Sort by date\n",
    "prices_df.sort_index(inplace=True)\n",
    "\n",
    "# 3: Ensure index is datetime object\n",
    "prices_df.index = pd.to_datetime(prices_df.index)\n",
    "\n",
    "prices_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Feature Reduction\n",
    "\n",
    "Note: The first block is also from the previous Notebook. Should probably add these to src."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(remove_white_spaces)\n",
    "    df[column_name] = df[column_name].apply(remove_non_alphanumeric)\n",
    "    df[column_name] = df[column_name].apply(remove_numbers)\n",
    "    df[column_name] = df[column_name].apply(remove_stop_words)\n",
    "    df[column_name] = df[column_name].apply(lemmatize_text)\n",
    "    return df\n",
    "\n",
    "def calculate_word_frequency(word_list, df):\n",
    "    d = {word: sum([1 if word in article else 0 for article in df.title.values])/df.shape[0] for word in word_list}\n",
    "    \n",
    "    return pd.Series(d, index = d.keys()).sort_values(ascending=False)\n",
    "\n",
    "def get_list_of_words(articles, cut_off):    \n",
    "    combined_titles = \" \".join(articles.title.values.tolist())\n",
    "\n",
    "    set_of_words = list(set(combined_titles.split(\" \")))\n",
    "    \n",
    "    set_of_words = [word for word in set_of_words if len(word) > 3]\n",
    "\n",
    "    word_frequency = calculate_word_frequency(set_of_words, articles)\n",
    "    set_of_words = word_frequency.loc[word_frequency > cut_off].index\n",
    "    return set_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "article_df = clean_text(article_df, \"title\")\n",
    "\n",
    "list_of_words = get_list_of_words(article_df, cut_off=0.01)\n",
    "\n",
    "print(len(list_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can go through the article titles and filter out all words that are not in the list_of_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_word_from_text(text, list_of_words):\n",
    "    return \" \".join([word for word in text.split(\" \") if word in list_of_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.title = article_df.title.apply(drop_word_from_text, args=(list_of_words,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can drop the columns \"ticker\" and \"article\" as they won't be needed. \n",
    "\n",
    "Further, it will be useful to have a column for each word with a value of True or False if the word exists in the title or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = article_df.drop([\"ticker\", \"article\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8433, 256)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in list_of_words:\n",
    "    article_df[word] = article_df.title.str.contains(word)\n",
    "    \n",
    "article_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>mark</th>\n",
       "      <th>market</th>\n",
       "      <th>search</th>\n",
       "      <th>research</th>\n",
       "      <th>chan</th>\n",
       "      <th>hand</th>\n",
       "      <th>researchandmarkets</th>\n",
       "      <th>global</th>\n",
       "      <th>...</th>\n",
       "      <th>micro</th>\n",
       "      <th>american</th>\n",
       "      <th>unite</th>\n",
       "      <th>stage</th>\n",
       "      <th>cure</th>\n",
       "      <th>strategy</th>\n",
       "      <th>light</th>\n",
       "      <th>next</th>\n",
       "      <th>receive</th>\n",
       "      <th>administration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>pharmaceutical present annual global healthcar...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-18</td>\n",
       "      <td>pharmaceutical present phase result treatment ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>grow company award</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>pharmaceutical present america health care con...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>disease pipeline review insight researchandmar...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time                                              title   mark  \\\n",
       "0 2019-06-04  pharmaceutical present annual global healthcar...  False   \n",
       "1 2019-05-18  pharmaceutical present phase result treatment ...  False   \n",
       "2 2019-05-15                                 grow company award  False   \n",
       "3 2019-05-07  pharmaceutical present america health care con...  False   \n",
       "4 2019-05-02  disease pipeline review insight researchandmar...   True   \n",
       "\n",
       "   market  search  research   chan   hand  researchandmarkets  global  \\\n",
       "0   False   False     False  False  False               False    True   \n",
       "1   False   False     False  False  False               False   False   \n",
       "2   False   False     False  False  False               False   False   \n",
       "3   False   False     False  False  False               False   False   \n",
       "4    True    True      True   True   True                True   False   \n",
       "\n",
       "        ...        micro  american  unite  stage   cure  strategy  light  \\\n",
       "0       ...        False     False  False  False  False     False  False   \n",
       "1       ...        False      True  False  False  False     False  False   \n",
       "2       ...        False     False  False  False  False     False  False   \n",
       "3       ...        False     False  False  False  False     False  False   \n",
       "4       ...        False     False  False  False  False     False  False   \n",
       "\n",
       "    next  receive  administration  \n",
       "0  False    False           False  \n",
       "1  False    False           False  \n",
       "2  False    False           False  \n",
       "3  False    False           False  \n",
       "4  False    False           False  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## Build N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(words, n):\n",
    "    return combinations(words, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_set_of_articles_for_ngram(articles, ngram_tuple):\n",
    "    temp_articles = articles.copy()\n",
    "    for word in ngram_tuple:\n",
    "        temp_articles = temp_articles.loc[temp_articles[word]]\n",
    "    return temp_articles.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ngram = n_gram(list_of_words, 2)\n",
    "\n",
    "dict_linking_ngram_to_indexes = {word_tuple: find_set_of_articles_for_ngram(article_df, item) for word_tuple in ngram}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a very large number of pairs. Is it necessary to check them all? How could we further reduce the number of words? Perhaps can go back to the idea of frequency to remove all pairs of words with a low frequency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32131"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_linking_ngram_to_indexes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
